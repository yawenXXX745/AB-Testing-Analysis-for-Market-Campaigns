{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1e97fd-05bc-4959-9e4a-967283f29322",
   "metadata": {},
   "source": [
    "# Statistical Hypothesis\n",
    "\n",
    "The main goal in many research studies is to check whether the data collected support certain statements or predictions. \n",
    "* Statistical Hypothesis is a conjecture about a population parameter, this conjecture may or may not be true.\n",
    "* EX: The mean income for a resident of Denver is equal to the mean income for a resident of Seattle.\n",
    "  * Population paramter: mean income\n",
    "  * One population consists of residents of Denver while the other consists of residents of Seattle\n",
    " \n",
    "### Null Hypothesis\n",
    "The null hypothesis is the default assumption we start within a statistical test. It states there's no effect, no difference, or no relationship between the groups we are comparing. \n",
    "\n",
    "#### Alternative Hypothesis\n",
    "Alternative hypothesis states that existence of a difference or relationship between the groups we're comparing.\n",
    "\n",
    "##### Why null hypothesis?\n",
    "* The null hypothesis provides the baseline for statistical inference.\n",
    "* When we run an experiment, we collect data and calculate a p-value.\n",
    "* If the p-value is below a significance threshold, we reject the null hypothesis, meaning the observed effect is unlikely to be due to chance.\n",
    "* If not, we fail to reject the null, meaning we don't have strong enough evidence to claim a real effect.\n",
    "\n",
    "#### INTERVIEW RELATED\n",
    "The null hypothesis is the assumption that nothing has changed — that the treatment has no effect compared to the control. In A/B testing, it means both groups have the same metric values, and any differences we see are just due to random variation. We test against this baseline to decide whether the observed effect is statistically significant.\n",
    "\n",
    "### Design the study\n",
    "After stating the hypothesis, the researcher designs the study.\n",
    "* Select the correct statistical test.\n",
    "  * Statistical test -- uses the data obtained from a sample to make a decision about whether the null hypothesis should be rejected.\n",
    "* Choose an appropriate level of significance\n",
    "* Formulate a plan for conducting the study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd4ffe-5127-491d-984c-f9d85c2cb28f",
   "metadata": {},
   "source": [
    "### Type I and Type II Error\n",
    "#### Type I Error\n",
    "    reject H0 when H0 is true. Conclude that results are statistically significant, however, in reality, they came about purely by chance or because of unrelated factors. \n",
    "Denoted as $\\alpha$.\n",
    "\n",
    "#### Type II Error\n",
    "    do not reject H0 when H0 is false. \n",
    "Denoted as $\\beta$.\n",
    "\n",
    "#### P-Value\n",
    "    The P-value is the probability of observing data at least as extreme as what you actually saw, assuming the null hypothesis is true.\n",
    "\n",
    "Simpler words: If there really no effect, how surprising would my data be? \\\n",
    "Example: \\\n",
    "If A and B truely had the same conversion rate, what's the probability I'd see a difference this big or bigger just by random chance? (p-value)\n",
    "\n",
    "Interpreting p-value: \\\n",
    "Low p-value: The data are very unlikely under the null hypothesis --> we reject H0 --> evidence suggests a real effect. \\\n",
    "High p-value: The data are consistent with random chance under H0 --> we fail to reject H0 --> no strong evidence of a real effect.\n",
    "\n",
    "##### Assuming that Null Hypothesis is true, if low p-value, that is observed result falls into the region that is very unlikely to happen, we could consider reject H0.\n",
    "\n",
    "* The p-value does not tell you whether something is true.\n",
    "* When used, p-value need to be complemented by effect sizes and uncertainty (confidence intervals).\n",
    "* To answer 'How likely is this result to be true?', it is better to use Bayesian approach or a false-discovery rate together with the p-value to answer the question.\n",
    "\n",
    "#### INTERVIEW RELATED\n",
    "The p-value measures how likely your observed data (or more extreme) would be if there were truly no difference between your groups. In A/B testing, a low p-value means the observed difference is unlikely to be due to chance under the null hypothesis, so we may conclude the treatment had an effect.\n",
    "\n",
    "Imagine a curve showing the probability of each possible result if the null hypothesis were true. We pick a significance level (say 5%) and shade the extreme tails — those are the results we’d consider too unlikely under H₀. The p-value is the total probability of seeing a result as extreme or more extreme than our observed one. If the p-value lies within the shaded 5% region, we reject the null hypothesis; otherwise, we don’t.\n",
    "\n",
    "#### Significance Level\n",
    "* Significance level is a value that you set at the beginning of your study to assess the p-value, it denoted as $\\alpha$.\n",
    "* It explains the maximum probability of commiting a Type I error.\n",
    "\n",
    "### Statistical Significance & Practical Significance & Power\n",
    "Statistical Significance: shows that if an effect exists in a study, denoted by p-value. \\\n",
    "Practical Significance: shows that if an effect is large enough to be meaningful in the real world, denoted by effect size.\n",
    "\n",
    "#### Effect Size\n",
    "Effect size is a quantitative measure of the magnitude of a phenomenon. In the context of A/B testing or statistical hypothesis testing, it represents how large the difference or association is between two groups or variables, independent of sample size. While a p-value only tells you whether a difference is statistically significant, the effect size tells you how meaningful or practically important that difference is.\n",
    "\n",
    "* Statistically significance alone can be misleading because it is influenced by the sample size. Increasing the sample size always makes it more likely to find a statistically effect, no matter how small the effect truly is in the real world.\n",
    "* In contrast, effect sizes are independent of the sample size, related only to the data used to calculate effect size.\n",
    "* It would be better to report effect sizes, confidence intervals together with p-value to present the relationship or effect between those two groups.\n",
    "\n",
    "### Power\n",
    "Power is the probability of correctly reject H0 when H1 is true. \\\n",
    "Type II Error ($\\beta$)is the probability of failing to reject H0 when H1 is true. \\\n",
    "Type I error ($\\alpha$)is the probability of rejecting H0 when H0 is true.\n",
    "\n",
    "Therefore, Power = 1 - $\\beta$, risk of type II error ($\\beta$) is inversely related to the statistical power of study. \n",
    "\n",
    "#### Statistical power is determined by...\n",
    "* Effect sizes: Larger effects are more easily detected.\n",
    "* Sample size: Larger samples reduce sampling error and increase power.\n",
    "* Significance level: Increase the significance level increases the power.\n",
    "* Measurement error: Systemetic and random errors in recorded data reduce power.\n",
    "\n",
    "#### Type I Error is even worse...\n",
    "A type I error means mistakenly going against the main statistical assumption of a null hypothesis. This may lead to new policies, practices or treatments that are inadequate or a waste of resources. \\\n",
    "In practical terms, however, either type of error could be worse depending on your research context.\n",
    "\n",
    "#### How to reduce Type I Error\n",
    "* The risk of making type I error is the significance level (or alpha) that you choose. That's a value that you set at the beginning of your study to assess the statistical probability of obtaining your results (p-value).\n",
    "* The significance level is usually set at 0.05, This means that your results only have a 5% chance of occuring, or less, if the null hypothesis is actually true.\n",
    "* To reduce the type I error probability, you can set a lower significance level.\n",
    "\n",
    "#### How to reduce Type II Error\n",
    "*  The risk of making type II error is inversely related to the statistical power of a test. Power is the extent to which a test can correctly detect a real effect when there's one.\n",
    "*  To (indirectly) reduce the risk of a type II error, you can increase the sample size or the significance level to increase the statistical power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80125afd-48db-4d80-b4cc-148e46bde4d4",
   "metadata": {},
   "source": [
    "## Effect Size and Power\n",
    "\n",
    "#### What happens to p-value as sample size increases?\n",
    "Note that unless $\\beta$ is exactly equal to 0 with an infinite number of decimals (in which case the p-value will approach 1), the p-value will approach 0. A similar mathametical relationship exists between the test statistic and the sample size in all statistical tests, including regression models with multiple independent variables. \n",
    "\n",
    "#### Statistical Significance (p-value)\n",
    "* Hypothesis testing traditionally focused on p-values to derive statistical significance when alpha is less than 0.05 has a major weakness.\n",
    "* With a large enough sample size any experiment can eventually reject the null hypothesis and detect trivially small differences that turn out to be statistically significant.\n",
    "* This is the reason why drug companies structure clinical trials to obtain FDA approval with very large samples. The large sample size will reduce the standard error to close to zero. This in turn will artificially boost the t stat and commensurately lower the p-value to close to 0.\n",
    "\n",
    "### Practical Significance (Effect Sizes)\n",
    "Effect sizes tell you how meaningful the relationship between variables or the difference between groups is. \\\n",
    "A large effect size means that a research finding has practical significance, while a small effect size indicates limited practical applications. \n",
    "\n",
    "## Measures of Effect Size \n",
    "The measures of the effect sizes can be grouped into 3 categories, based on their approaches to define the effect. \\ \n",
    "The groups are:\n",
    "### Metrics based on the correlation (R Family)\n",
    "  * Most popular -- Pearson's R. Measures the degree of linear association between two real-valued variables.\n",
    "  * Coefficient of determination ($R^2$) \n",
    "    * It states what proportion of the dependent variable (y)'s variance is explained (predictable) by the independent variables (x). Generally, a higher $R^2$ indicates a better fit for the model.\n",
    "\n",
    "  When using simple linear regression (with one dependent variable) with the intercept included, the coefficient of determination ($R^2$) is simply the square of the Pearson's R. Due to the fact that we square the Pearson's R, the coefficient of determination does not convey any direction information of correlation.\n",
    "\n",
    "#### Coefficient of Determination $R^2$\n",
    "  $R^2$ is a comparison of residual sum of squares ($SS_{res}$) with total sum of squares ($SS_{tot}$)\n",
    "  $$ R^2 = 1 - \\frac{RR_{res}}{RR_{tot}}$$\n",
    "  where: \\\n",
    "  Total sum of squares is calculated by summation of squares of perpendicular distance between data points and average line. It represents the variation of the observed data.\n",
    "  \n",
    "  Residual sum of squares is calculated by summation of squares of perpendicular distance between data points and the best-fitted line. It measures how well the regression model represents the data.\n",
    "\n",
    "  If $R^2$ = 1, all of the data points fall perfectly on the regression line. The predictor x accounts for all of the variation in y ($SS_{res}$ = 0).\\\n",
    "  If $R^2$ = 0, the estimated regression line is perfectly horizontal. The predictor x accounts for none of the variation in y, therefore, there's no linear relationship between x and y.\n",
    "\n",
    "  $R^2$ expresses the proportion of the variation in Y that is caused by variation in X. R expresses the strength, direction and linearity in the relation between X and Y. The $R^2$ and R quantify the strength of a linear relationship, it is possible that $R^2$ = 0 and R = 0, suggesting there's no linear relationship between y and x, and yet a perfect curved exists. In simple linear regression, $R^2$ equals to Pearson's R square. However, for multiple or nonlinear regression, $R^2$ is defined as the proportion of variance explained. Besides, the $R^2$ and R can both be greatly affected by just one data point or a few data points (outliers). What's more, a statistically significant $R^2$ does not imply that the slope $\\beta1$ is meaningfully different from 0. Since the larger the dataset, the easier it is to reject null hypothesis and claim statistical significance, which does not imply a practical significance. \n",
    "\n",
    "### Metrics based on differences (D Family)        \n",
    "* Calculating the difference between the mean values of the samples. Usually the difference is standardized by dividing it by the standard deviation.\n",
    "* Effect sizes are independent from the sample size, because the unit of statistical distance or differentiation in effect size analysis is the standard deviation instead of the standard error. Standard deviation is completely independent from sample size. On the other hand, standard error is dependent from the sample size.\n",
    "\n",
    "#### Why standardized?\n",
    "* You can compare the standardized difference across variables.\n",
    "* You don't have to be familiar with the scaling of the variables.\n",
    "\n",
    "#### Cohen's d -- Standardized Mean Difference\n",
    "Cohen's d is one of the most common ways to measure effect size, the difference is expressed in terms of the number of standard deviations\n",
    "$$ d = \\frac{\\mu_1-\\mu_2}{s} $$\n",
    "$$     s = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}$$\n",
    "where, s is the pooled standard deviation, $s_1$ and $s_2$ are standard deviations of the two independent samples.\n",
    "\n",
    "* There's considerable overlap between the two distribution even when Cohen'd indicates a large effect size.\n",
    "* Cohen's d: values of d across disciplines: In psychology, the mean effect sizes is 0.4, with 30% of effects below 0.2 and 17% greater than 0.8. However, the average effect sizes is also d=0.4, with 0.2, 0.4, 0.6 considered small, medium, large effect sizes. Medical research is often associated with small effect sizes, often in the 0.05 to 0.2 range. Despite being small, these effects often represent meaningful effects such as saving lives.\n",
    "* Cohen's d is very frequently used in estimating the required sample size for A/B test. In general, a lower value of Cohen'd indicates the necessity of a larger sample size and vice versa. \n",
    "### Metrics for categorical variables\n",
    "#### Odds\n",
    "* The odds of an event is a ratio of the frequency (or likelihood) of its occurrence to the frequency (or likelihood) of its non-occurence.\n",
    "#### Odds Ratio\n",
    "* The odds ratio is comparison of the odds of an event after exposure to a risk factor with the odds of that event in a control or reference situation. That is, the odds of an event in Treatment group divided by the odds of that event in Control group.\n",
    "  $$\\frac{p_1/(1-p_1)}{p_2/(1-p_2)} = \\frac{p_1/q_1}{p_2/q_2} = \\frac{p_1q_2}{p_2q_1}$$\n",
    "* An odds ratio of...\n",
    "  * 1.0 (or close to 1.0) indicates that X is not associated with y, the odds of the event are the same between groups.\n",
    "  * greater than 1.0 indicates a positive association, the event odds increase compared to the reference group (or per unit increase). EX. OR=2 means the odds are twice as high.\n",
    "  * smaller than 1.0 indicates a negative association, the event odds decrease compared to the reference group. EX. OR=0.5 means the odds are half as high. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb6aa5-a7d5-465c-acbf-081e525f0ee2",
   "metadata": {},
   "source": [
    "## Power Analysis\n",
    "Power Analysis is built from \n",
    "##### significance level, effect size, power, sample size. \n",
    "All four of these variables are linked together and changing one of them impacts the others. Following this relationship, if three of these variables are known then we can determine the fourth unknown variable, and this is what power analysis is all about. \n",
    "\n",
    "### Power analysis is in...\n",
    "* Experiment Design\n",
    "  * Select the alpha, power and effect size that is relevant for the experiment, and consequently calculate the sample size that will be needed for such an experiment.\n",
    "  * Use tt_ind_solve_power() function in statsmodels. Requires\n",
    "  * ##### effect_size(standardized effect size), alpha, power, ratio. In addition, alternative: Power the test to detect two-sided effects. Returns an estimated sample size\n",
    "\n",
    "* Validate the findings of an experiment\n",
    "  * By using the given sample size, effect_size and significance level, you can\n",
    "    ##### determine the power of the conducted experiment\n",
    "    to conclude whether the probability of committing a Type II error is acceptable from the decision-making perspective.\n",
    "\n",
    "* Sensitivity Test\n",
    "  * Analyze the impact of changing one variable on the rest of the three. The results can be plotted on a graph to explain the behaviour of the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333e96d-08e3-4588-ae0f-09d90d9c9cdf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* Statistical power is the probability that a test will correctly reject null hypothesis. Statistical power has relevance only when the null hypothesis is false.\n",
    "* The higher the statistical power for a given experiment, the lower the probability of making type II error.\n",
    "* ##### High Statistical Power: Small risk of committing type II error\n",
    "* It is common to design experiments with a statistical power of 80% or higher, this means that only 20% probability of encountering a type II error.\n",
    "* Power Analysis answers questions like 'how much statistical power does my study have?' and 'how big a sample size do I need?'. Power analysis are normally run before a study is conducted. A prospective or a priori power analysis can be used to estimate any one of the four power parameters but is most often used to estimate\n",
    "##### required sample sizes.\n",
    "* As a practitioner, we can start with sensible defaults for some parameters, such as significance level of 0.05, and a power level of 0.80, then we can estimate a desirable minimum effect size, specific to the experiment being performed. Finally, we can estimate a minimum sample size based on all the other parameters in the power analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
